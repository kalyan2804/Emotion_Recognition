{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition by Textual Tweets Classification Using 7 algo's including Voting Classifier (LR-SGD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file \n",
    "df=pd.read_excel('E:\\original_data_set.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the imported Data set File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_extrction_time_index</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Twitter_body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet_extrction_time_index  Emotion  \\\n",
       "0                           1        0   \n",
       "1                           2        0   \n",
       "2                           3        1   \n",
       "3                           4        0   \n",
       "4                           5        0   \n",
       "\n",
       "                                   Twitter_body_text  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the top 5 rows of the data frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_extrction_time_index</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Twitter_body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>99996</td>\n",
       "      <td>0</td>\n",
       "      <td>@Cupcake  seems like a repeating problem   hop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>99997</td>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake__ arrrr we both replied to each other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "      <td>@CuPcAkE_2120 ya i thought so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>@Cupcake_Dollie Yes. Yes. I'm glad you had mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>100000</td>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake_kayla haha yes you do</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tweet_extrction_time_index  Emotion  \\\n",
       "99984                       99996        0   \n",
       "99985                       99997        1   \n",
       "99986                       99998        0   \n",
       "99987                       99999        1   \n",
       "99988                      100000        1   \n",
       "\n",
       "                                       Twitter_body_text  \n",
       "99984  @Cupcake  seems like a repeating problem   hop...  \n",
       "99985  @cupcake__ arrrr we both replied to each other...  \n",
       "99986                     @CuPcAkE_2120 ya i thought so   \n",
       "99987  @Cupcake_Dollie Yes. Yes. I'm glad you had mor...  \n",
       "99988                    @cupcake_kayla haha yes you do   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the bottom 5 rows of the data frame\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99989, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#understanding the no. of rows and columns of Data Frame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tweet_extrction_time_index', 'Emotion', 'Twitter_body_text']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Listing all the columns of the data frame\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet_extrction_time_index     int64\n",
       "Emotion                        int64\n",
       "Twitter_body_text             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Understanding the Columns data Types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56457, 43532]\n"
     ]
    }
   ],
   "source": [
    "list_y=[]\n",
    "list_y.append(int(df.Emotion.value_counts()[1]))\n",
    "list_y.append(int(df.Emotion.value_counts()[0]))\n",
    "print(list_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "list_x=['Happy','Unhappy']\n",
    "list_y=[]\n",
    "list_y.append(int(df.Emotion.value_counts()[1]))\n",
    "list_y.append(int(df.Emotion.value_counts()[0]))\n",
    "ax.bar(list_x,list_y)\n",
    "plt.xlabel(\"Emotion\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)Removing the unnecessary characters\n",
    "- helps us in removal of Un-necessary things like emojis's, differnet languages , symbols and stuff\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_1=[]\n",
    "for each_row in range(df.shape[0]):\n",
    "    text=df['Twitter_body_text'].values[each_row]\n",
    "    tempo=str(text)\n",
    "    tempe=re.sub('[^A-Za-z0-9!?]', ' ', tempo)\n",
    "    clean_1.append(tempe)\n",
    "df[\"cleaned_1\"]=clean_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg its already 7 30  O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Omgaga  Im sooo  im gunna CRy  I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>Cupcake  seems like a repeating problem   hop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>cupcake   arrrr we both replied to each other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>CuPcAkE 2120 ya i thought so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>Cupcake Dollie Yes  Yes  I m glad you had mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>cupcake kayla haha yes you do</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               cleaned_1\n",
       "0                           is so sad for my APL frie...\n",
       "1                         I missed the New Moon trail...\n",
       "2                                omg its already 7 30  O\n",
       "3                   Omgaga  Im sooo  im gunna CRy  I ...\n",
       "4               i think mi bf is cheating on me!!!   ...\n",
       "...                                                  ...\n",
       "99984   Cupcake  seems like a repeating problem   hop...\n",
       "99985   cupcake   arrrr we both replied to each other...\n",
       "99986                      CuPcAkE 2120 ya i thought so \n",
       "99987   Cupcake Dollie Yes  Yes  I m glad you had mor...\n",
       "99988                     cupcake kayla haha yes you do \n",
       "\n",
       "[99989 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['cleaned_1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)Removing the punctuation and Numbers\n",
    "- as they don't have anything to do with determining the emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_2=[]\n",
    "for each_row in range(df.shape[0]):\n",
    "    text=df['cleaned_1'].values[each_row]\n",
    "    tempo_c1=str(text)\n",
    "    tempe_c1=re.sub('[^A-Za-z]', ' ', tempo_c1)\n",
    "    clean_2.append(tempe_c1)\n",
    "df[\"cleaned_2\"]=clean_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg its already       O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Omgaga  Im sooo  im gunna CRy  I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i think mi bf is cheating on me      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>Cupcake  seems like a repeating problem   hop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>cupcake   arrrr we both replied to each other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>CuPcAkE      ya i thought so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>Cupcake Dollie Yes  Yes  I m glad you had mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>cupcake kayla haha yes you do</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               cleaned_2\n",
       "0                           is so sad for my APL frie...\n",
       "1                         I missed the New Moon trail...\n",
       "2                                omg its already       O\n",
       "3                   Omgaga  Im sooo  im gunna CRy  I ...\n",
       "4               i think mi bf is cheating on me      ...\n",
       "...                                                  ...\n",
       "99984   Cupcake  seems like a repeating problem   hop...\n",
       "99985   cupcake   arrrr we both replied to each other...\n",
       "99986                      CuPcAkE      ya i thought so \n",
       "99987   Cupcake Dollie Yes  Yes  I m glad you had mor...\n",
       "99988                     cupcake kayla haha yes you do \n",
       "\n",
       "[99989 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['cleaned_2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Converting the cleaned_2 description of tweets in to lower case\n",
    "- basically making sure that the text doesn't consists of any upper case letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_3=[]\n",
    "for each_row in range(df.shape[0]):\n",
    "    text=df['cleaned_2'].values[each_row]\n",
    "    tempo_c2=str(text)\n",
    "    tempe_c2=tempo_c2.lower()\n",
    "    clean_3.append(tempe_c2)\n",
    "df[\"cleaned_3\"]=clean_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is so sad for my apl frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i missed the new moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg its already       o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>omgaga  im sooo  im gunna cry  i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i think mi bf is cheating on me      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>cupcake  seems like a repeating problem   hop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>cupcake   arrrr we both replied to each other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>cupcake      ya i thought so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>cupcake dollie yes  yes  i m glad you had mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>cupcake kayla haha yes you do</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               cleaned_3\n",
       "0                           is so sad for my apl frie...\n",
       "1                         i missed the new moon trail...\n",
       "2                                omg its already       o\n",
       "3                   omgaga  im sooo  im gunna cry  i ...\n",
       "4               i think mi bf is cheating on me      ...\n",
       "...                                                  ...\n",
       "99984   cupcake  seems like a repeating problem   hop...\n",
       "99985   cupcake   arrrr we both replied to each other...\n",
       "99986                      cupcake      ya i thought so \n",
       "99987   cupcake dollie yes  yes  i m glad you had mor...\n",
       "99988                     cupcake kayla haha yes you do \n",
       "\n",
       "[99989 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['cleaned_3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4). Tokenizing and Removing the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary packages\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "stopwords_list=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing and remocving stop words\n",
    "mlist=[]\n",
    "for each_row in range(df.shape[0]):\n",
    "    text=df['cleaned_3'].values[each_row]\n",
    "    tokenized_words=word_tokenize(text)\n",
    "    clean_words=[]\n",
    "    for each_word in tokenized_words:\n",
    "        if not each_word in stopwords_list:\n",
    "            clean_words.append(each_word)\n",
    "    dre=''\n",
    "    for each_data in clean_words:\n",
    "        dre=dre+str(each_data)+' '\n",
    "    mlist.append(dre)\n",
    "df['cleaned_4']=mlist\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sad apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>missed new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>omgaga im sooo im gunna cry dentist since supo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>think mi bf cheating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>cupcake seems like repeating problem hope able...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>cupcake arrrr replied different tweets time se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>cupcake ya thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>cupcake dollie yes yes glad fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>cupcake kayla haha yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               cleaned_4\n",
       "0                                        sad apl friend \n",
       "1                               missed new moon trailer \n",
       "2                                           omg already \n",
       "3      omgaga im sooo im gunna cry dentist since supo...\n",
       "4                                  think mi bf cheating \n",
       "...                                                  ...\n",
       "99984  cupcake seems like repeating problem hope able...\n",
       "99985  cupcake arrrr replied different tweets time se...\n",
       "99986                                cupcake ya thought \n",
       "99987                   cupcake dollie yes yes glad fun \n",
       "99988                            cupcake kayla haha yes \n",
       "\n",
       "[99989 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['cleaned_4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5). converting all the words in to their root words using either lemmatizer/stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary_packages\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lm=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_5=[]\n",
    "for each_row in range(df.shape[0]):\n",
    "    text=df['cleaned_4'].values[each_row]\n",
    "    tokenized_words=word_tokenize(text)\n",
    "    clean_words=[]\n",
    "    for each_word in tokenized_words:\n",
    "        clean_words.append(lm.lemmatize(each_word))\n",
    "    dre=''\n",
    "    for each_data in clean_words:\n",
    "        dre=dre+str(each_data)+' '\n",
    "    clean_5.append(dre)\n",
    "df['cleaned_5']=clean_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sad apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>missed new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>omgaga im sooo im gunna cry dentist since supo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>think mi bf cheating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>cupcake seems like repeating problem hope able...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>cupcake arrrr replied different tweet time see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>cupcake ya thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>cupcake dollie yes yes glad fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>cupcake kayla haha yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               cleaned_5\n",
       "0                                        sad apl friend \n",
       "1                               missed new moon trailer \n",
       "2                                           omg already \n",
       "3      omgaga im sooo im gunna cry dentist since supo...\n",
       "4                                  think mi bf cheating \n",
       "...                                                  ...\n",
       "99984  cupcake seems like repeating problem hope able...\n",
       "99985  cupcake arrrr replied different tweet time see...\n",
       "99986                                cupcake ya thought \n",
       "99987                   cupcake dollie yes yes glad fun \n",
       "99988                            cupcake kayla haha yes \n",
       "\n",
       "[99989 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['cleaned_5']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6)Dropping the null value row's if any present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['cleaned_5']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99957, 8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking shape after removing\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Forming the new_data set with the finally cleaned tweets and emotion for simplification of usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "per=list()\n",
    "per=df['cleaned_5'].to_list()\n",
    "df_2 = pd.DataFrame()\n",
    "df_2['Final_clean_Tweets_text']=per\n",
    "df_2['Sentiment']=df['Emotion'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_clean_Tweets_text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sad apl friend</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>missed new moon trailer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg already</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>omgaga im sooo im gunna cry dentist since supo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>think mi bf cheating</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99952</th>\n",
       "      <td>cupcake seems like repeating problem hope able...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99953</th>\n",
       "      <td>cupcake arrrr replied different tweet time see...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99954</th>\n",
       "      <td>cupcake ya thought</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99955</th>\n",
       "      <td>cupcake dollie yes yes glad fun</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99956</th>\n",
       "      <td>cupcake kayla haha yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99957 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Final_clean_Tweets_text  Sentiment\n",
       "0                                        sad apl friend           0\n",
       "1                               missed new moon trailer           0\n",
       "2                                           omg already           1\n",
       "3      omgaga im sooo im gunna cry dentist since supo...          0\n",
       "4                                  think mi bf cheating           0\n",
       "...                                                  ...        ...\n",
       "99952  cupcake seems like repeating problem hope able...          0\n",
       "99953  cupcake arrrr replied different tweet time see...          1\n",
       "99954                                cupcake ya thought           0\n",
       "99955                   cupcake dollie yes yes glad fun           1\n",
       "99956                            cupcake kayla haha yes           1\n",
       "\n",
       "[99957 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting in to a Vector_matrix_form using TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary package's\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99957, 92221)\n"
     ]
    }
   ],
   "source": [
    "transformed_matrix=tf_idf.fit_transform(df_2['Final_clean_Tweets_text'])\n",
    "print(transformed_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verifying the Sentiment Column Data type before proceeding further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2['Sentiment'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(transformed_matrix,df_2['Sentiment'],test_size=0.30,train_size=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69969, 92221), (69969,), (29988, 92221), (29988,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1=rfc.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1=accuracy_score(y_pred_1,y_test)\n",
    "rf2=precision_score(y_pred_1,y_test)\n",
    "rf3=recall_score(y_pred_1,y_test)\n",
    "rf4=f1_score(y_pred_1,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73062558356676 0.7973638900433414 0.7421939762365294 0.7687904287595169\n"
     ]
    }
   ],
   "source": [
    "print(rf1,rf2,rf3,rf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages\n",
    "from sklearn.svm import SVC\n",
    "svc=SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2=svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv1=accuracy_score(y_pred_2,y_test)\n",
    "sv2=precision_score(y_pred_2,y_test)\n",
    "sv3=recall_score(y_pred_2,y_test)\n",
    "sv4=f1_score(y_pred_2,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.751734026944111 0.8509766668645729 0.7438758563421216 0.7938301348619535\n"
     ]
    }
   ],
   "source": [
    "print(sv1,sv2,sv3,sv4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_3=nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb1=accuracy_score(y_pred_3,y_test)\n",
    "nb2=precision_score(y_pred_3,y_test)\n",
    "nb3=recall_score(y_pred_3,y_test)\n",
    "nb4=f1_score(y_pred_3,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nb1,nb2,nb3,nb4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc=GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_4=gbc.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc1=accuracy_score(y_pred_4,y_test)\n",
    "gbc2=precision_score(y_pred_4,y_test)\n",
    "gbc3=recall_score(y_pred_4,y_test)\n",
    "gbc4=f1_score(y_pred_4,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbc1,gbc2,gbc3,gbc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lrc=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madir\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_5=lrc.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc1=accuracy_score(y_pred_5,y_test)\n",
    "lrc2=precision_score(y_pred_5,y_test)\n",
    "lrc3=recall_score(y_pred_5,y_test)\n",
    "lrc4=f1_score(y_pred_5,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7504001600640257 0.8367648798958703 0.7495362764322434 0.7907522853708311\n"
     ]
    }
   ],
   "source": [
    "print(lrc1,lrc2,lrc3,lrc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(loss='log')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_6=sgd.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd1=accuracy_score(y_pred_6,y_test)\n",
    "sgd2=precision_score(y_pred_6,y_test)\n",
    "sgd3=recall_score(y_pred_6,y_test)\n",
    "sgd4=f1_score(y_pred_6,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.737328264639189 0.8649272275470359 0.7232474150299313 0.7877677488885896\n"
     ]
    }
   ],
   "source": [
    "print(sgd1,sgd2,sgd3,sgd4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_pred_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic_Regression Probability Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred_7_list=[]\n",
    "for each_record in x_test:\n",
    "    probi1=lrc.predict_proba(each_record)\n",
    "    probi2=sgd.predict_proba(each_record)\n",
    "    lr_sgd_neg=(probi1[0][0])+(probi2[0][0])/2\n",
    "    lr_sgd_pos=(probi1[0][1])+(probi2[0][1])/2\n",
    "    if(lr_sgd_neg>lr_sgd_pos):\n",
    "        y_pred_7_list.append(0)\n",
    "    else:\n",
    "        y_pred_7_list.append(1)\n",
    "y_pred_7= np.array(y_pred_7_list)\n",
    "print(y_pred_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc1=accuracy_score(y_pred_7,y_test)\n",
    "vc2=precision_score(y_pred_7,y_test)\n",
    "vc3=recall_score(y_pred_7,y_test)\n",
    "vc4=f1_score(y_pred_7,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7486994797919168 0.8441604543840966 0.7442879499217527 0.791084497671324\n"
     ]
    }
   ],
   "source": [
    "print(vc1,vc2,vc3,vc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
